{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPsONuSS-5kq",
        "outputId": "7886037c-055c-4628-b49e-f9ddd1616b46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /kaggle/input/faceforensics\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"hungle3401/faceforensics\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS4QX2p1i6OJ",
        "outputId": "b4af954c-a6cb-4441-83d5-237a5649bfe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))  # Should show 1+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 1: Setup and Install Tools "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1GUtux-m_YEH",
        "outputId": "324d4672-09a9-4c3c-c02e-d74a03140c63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import ViTForImageClassification, ViTImageProcessor  # Updated to avoid deprecation\n",
        "import cv2\n",
        "from mtcnn import MTCNN\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm  \n",
        "\n",
        "# Set device (GPU in Kaggle)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 2: Data Preparation (With Progress and Visuals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "DRTyDrLYGtyj",
        "outputId": "273c617f-ef18-418e-acf8-0d5602dfe8eb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define your OWN directories for real and fake videos\n",
        "# Make sure these paths are correct based on your Kaggle environment\n",
        "\n",
        "real_dir = r'archive\\FF++\\real'  # Folder with real videos    \n",
        "fake_dir = r'archive\\FF++\\fake'  # Folder with fake videos\n",
        "\n",
        "# Extracting faces \n",
        "def extract_faces_from_video(video_path, num_frames=10):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    detector = MTCNN()\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    step = max(1, frame_count // num_frames)\n",
        "\n",
        "    for i in range(0, frame_count, step):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        faces = detector.detect_faces(frame_rgb)\n",
        "        if faces:\n",
        "            x, y, w, h = faces[0]['box']\n",
        "            face = frame_rgb[y:y+h, x:x+w]\n",
        "            face = cv2.resize(face, (224, 224))\n",
        "            frames.append(Image.fromarray(face))\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "# Imports for progress and visualization\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Updated Dataset class with progress and limit ( 50 videos per folder for speed) - now single-threaded\n",
        "# Multithredding needs to be avoided in Kaggle due to resource limits\n",
        "\n",
        "class DeepfakeDataset(Dataset):\n",
        "    def __init__(self, image_processor, max_videos_per_class=100):   # Currently set to 100 for accuracy\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.image_processor = image_processor\n",
        "\n",
        "        # Process real videos (single-threaded with progress)\n",
        "        real_files = [f for f in os.listdir(real_dir) if f.endswith('.mp4')][:max_videos_per_class]\n",
        "        print(\"Processing real videos (single-threaded)...\")\n",
        "        for file in tqdm(real_files):\n",
        "            frames = extract_faces_from_video(os.path.join(real_dir, file))\n",
        "            self.images.extend(frames)\n",
        "            self.labels.extend([0] * len(frames))  # 0 = real\n",
        "\n",
        "        # Process fake videos (single-threaded with progress)\n",
        "        fake_files = [f for f in os.listdir(fake_dir) if f.endswith('.mp4')][:max_videos_per_class]\n",
        "        print(\"Processing fake videos (single-threaded)...\")\n",
        "        for file in tqdm(fake_files):\n",
        "            frames = extract_faces_from_video(os.path.join(fake_dir, file))\n",
        "            self.images.extend(frames)\n",
        "            self.labels.extend([1] * len(frames))  # 1 = fake\n",
        "\n",
        "        print(f\"Extracted {len(self.images)} face images from videos.\")\n",
        "\n",
        "        # Showing num of fake and real images \n",
        "        num_real = sum(1 for label in self.labels if label == 0)\n",
        "        num_fake = sum(1 for label in self.labels if label == 1)\n",
        "        print(f\"Class Balance: Real images: {num_real}, Fake images: {num_fake}\")\n",
        "\n",
        "        # Visualize a sample\n",
        "        if self.images:\n",
        "            sample_img = self.images[0]\n",
        "            plt.imshow(sample_img)\n",
        "            plt.title(\"Sample Face (Real)\" if self.labels[0] == 0 else \"Sample Face (Fake)\")\n",
        "            plt.show()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        inputs = self.image_processor(img, return_tensors='pt').pixel_values.squeeze(0)  # Updated\n",
        "        return {'pixel_values': inputs, 'labels': self.labels[idx]}\n",
        "\n",
        "# Load image processor \n",
        "image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "# Create dataset (limit to 50 videos each for speed)\n",
        "# Currently set to 100 for accuracy\n",
        "dataset = DeepfakeDataset(image_processor, max_videos_per_class=100)\n",
        "\n",
        "# Split 80/20\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "print(f\"Total images: {len(dataset)} (Train: {train_size}, Test: {test_size})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 3: Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G27s6fsnG854",
        "outputId": "7ea1b46a-ef57-4a16-ab52-ac35ed4ba50a"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ConViTInspired(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ConViTInspired, self).__init__()\n",
        "        self.vit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=num_classes, ignore_mismatched_sizes=True)\n",
        "\n",
        "        # Convolutional layer to extract features (expands to 16 channels, inspired by paper)\n",
        "        self.conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # New: Projection layer to reduce back to 3 channels (fixes the error!)\n",
        "        self.proj = nn.Conv2d(16, 3, kernel_size=1)  # 1x1 conv to match ViT input\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, pixel_values, labels=None):\n",
        "        x = self.conv(pixel_values)  # Apply conv (now 16 channels)\n",
        "        x = self.relu(x)\n",
        "        x = self.proj(x)  # Project back to 3 channels\n",
        "        outputs = self.vit(pixel_values=x, labels=labels)  # Now it matches ViT's expectation\n",
        "        return outputs\n",
        "\n",
        "# Create model and move to device\n",
        "model = ConViTInspired(num_classes=2).to(device)\n",
        "\n",
        "# layers for fine-tuning \n",
        "for param in model.vit.parameters():\n",
        "    param.requires_grad = True  # UnFreeze ViT base\n",
        "for param in model.vit.classifier.parameters():\n",
        "    param.requires_grad = True  # Unfreeze classifier\n",
        "for param in model.conv.parameters():\n",
        "    param.requires_grad = True  # Train the conv layer\n",
        "for param in model.proj.parameters():  # New: Train the projection layer too\n",
        "    param.requires_grad = True\n",
        "\n",
        "print(\"Model loaded with pretrained weights and channel fix!\")\n",
        "print(\"Model ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 4: Fine-Tuning with Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "VSqDFeONHCHk",
        "outputId": "22f5252c-0e1e-4ef6-ed05-96c315dc11ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recalculated Class Balance for Weights: Real: 1052, Fake: 1083\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 214/214 [15:05<00:00,  4.23s/it, loss=0.355]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Avg Loss: 0.6021\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 214/214 [16:18<00:00,  4.57s/it, loss=0.866] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Avg Loss: 0.3747\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 214/214 [16:07<00:00,  4.52s/it, loss=0.0816] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Avg Loss: 0.2084\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 214/214 [16:24<00:00,  4.60s/it, loss=0.0677] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Avg Loss: 0.1540\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 214/214 [16:22<00:00,  4.59s/it, loss=0.126]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 Avg Loss: 0.0876\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 214/214 [16:16<00:00,  4.56s/it, loss=0.0554]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 Avg Loss: 0.0691\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 214/214 [16:08<00:00,  4.52s/it, loss=0.0003]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 Avg Loss: 0.0785\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 214/214 [16:21<00:00,  4.59s/it, loss=0.00715] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 Avg Loss: 0.0666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 214/214 [16:25<00:00,  4.61s/it, loss=2.5e-5]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 Avg Loss: 0.0220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|██████████| 214/214 [16:22<00:00,  4.59s/it, loss=0.00217] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 Avg Loss: 0.0477\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-5)  # Lower LR for stability\n",
        "\n",
        "# New: Recalculate class counts from the dataset (fixes NameError)\n",
        "num_real = sum(1 for label in dataset.labels if label == 0)\n",
        "num_fake = sum(1 for label in dataset.labels if label == 1)\n",
        "print(f\"Recalculated Class Balance for Weights: Real: {num_real}, Fake: {num_fake}\")\n",
        "\n",
        "# New: Add class weights to handle imbalance (weight fakes higher if fewer)\n",
        "if num_real == 0 or num_fake == 0:\n",
        "    class_weights = torch.tensor([1.0, 1.0]).to(device)  # Fallback if one class is empty\n",
        "else:\n",
        "    class_weights = torch.tensor([1.0, num_fake / num_real] if num_real > num_fake else [num_real / num_fake, 1.0]).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)  # Weighted loss\n",
        "\n",
        "losses = []\n",
        "num_epochs = 10  # Increase for better training\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "    for batch in progress_bar:\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(pixel_values, labels=labels)\n",
        "        loss = criterion(outputs.logits, labels)  # Use weighted criterion\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    losses.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1} Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Plot loss\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Progress')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 5: Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "W-k2BGInHHMd",
        "outputId": "9407f970-8db3-49de-f62a-1df420533d14"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "model.eval()\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(pixel_values)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Real', 'Fake'])\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(true_labels, pred_labels)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 6: Predict on a Single Video (Upload your own via /kaggle/input or add a path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1gL26_THMkw",
        "outputId": "8a6a948d-56fe-4102-82b1-57a930143a5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction for archive/FF++/fake/01_11__talking_against_wall__9229VVZ3.mp4: Fake\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def predict_video(video_path):\n",
        "    frames = extract_faces_from_video(video_path, num_frames=5)\n",
        "    if not frames:\n",
        "        return \"No faces\"\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for frame in frames:\n",
        "            inputs = image_processor(frame, return_tensors='pt').pixel_values.to(device)\n",
        "            outputs = model(inputs)\n",
        "            pred = torch.argmax(outputs.logits, dim=1).item()\n",
        "            preds.append(pred)\n",
        "    avg_pred = np.mean(preds) > 0.5\n",
        "    return \"Fake\" if avg_pred else \"Real\"\n",
        "\n",
        "# Example: Test on a video from dataset (change path)\n",
        "test_video =\"archive/FF++/fake/01_11__talking_against_wall__9229VVZ3.mp4\"\n",
        "print(f\"Prediction for {test_video}: {predict_video(test_video)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 7: Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKYb12DQHVAH",
        "outputId": "24bcaf5f-652f-461d-ea93-ddf6570257a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "torch.save(model.state_dict(), 'my_deepfake_model.pth')\n",
        "print(\"Model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load and Use the Saved Model for further testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "Prediction for archive/FF++/fake/01_11__talking_against_wall__9229VVZ3.mp4: Fake\n",
            "Prediction for /path/to/video1.mp4: No faces detected\n",
            "Prediction for /path/to/video2.mp4: No faces detected\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Re-define the model class (copied from Cell 3 - needed to load weights)\n",
        "class ConViTInspired(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ConViTInspired, self).__init__()\n",
        "        self.vit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=num_classes, ignore_mismatched_sizes=True)\n",
        "\n",
        "        # Convolutional layer to extract features (expands to 16 channels, inspired by paper)\n",
        "        self.conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Projection layer to reduce back to 3 channels\n",
        "        self.proj = nn.Conv2d(16, 3, kernel_size=1)  # 1x1 conv to match ViT input\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, pixel_values, labels=None):\n",
        "        x = self.conv(pixel_values)  # Apply conv (now 16 channels)\n",
        "        x = self.relu(x)\n",
        "        x = self.proj(x)  # Project back to 3 channels\n",
        "        outputs = self.vit(pixel_values=x, labels=labels)  # Now it matches ViT's expectation\n",
        "        return outputs\n",
        "\n",
        "# Step 2: Create a new instance of the model and load the saved weights\n",
        "model = ConViTInspired(num_classes=2)  # Create empty model\n",
        "model.load_state_dict(torch.load('my_deepfake_model.pth'))  # Load weights\n",
        "model.to(device)  \n",
        "model.eval()  # Set to evaluation mode (important for predictions)\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Step 3: Re-define predict_video if not already in this cell (copy from Cell 6)\n",
        "def predict_video(video_path, num_frames=5):\n",
        "    frames = extract_faces_from_video(video_path, num_frames=num_frames)\n",
        "    if not frames:\n",
        "        return \"No faces detected\"\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for frame in frames:\n",
        "            inputs = image_processor(frame, return_tensors='pt').pixel_values.to(device)\n",
        "            outputs = model(inputs)\n",
        "            pred = torch.argmax(outputs.logits, dim=1).item()\n",
        "            preds.append(pred)\n",
        "    avg_pred = np.mean(preds) > 0.5  # Standard threshold\n",
        "    return \"Fake\" if avg_pred else \"Real\"\n",
        "\n",
        "# Step 4: Example - Predict on a new video (change the path to your video)\n",
        "test_video_path = \"archive/FF++/fake/01_11__talking_against_wall__9229VVZ3.mp4\"  # Example fake video\n",
        "prediction = predict_video(test_video_path)\n",
        "print(f\"Prediction for {test_video_path}: {prediction}\")\n",
        "\n",
        "# Optional: Predict on multiple videos\n",
        "multiple_videos = [\n",
        "    \"/path/to/video1.mp4\",  # Add your video paths here\n",
        "    \"/path/to/video2.mp4\"\n",
        "]\n",
        "for vid in multiple_videos:\n",
        "    print(f\"Prediction for {vid}: {predict_video(vid)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 8: Create Web App Utilities\n",
        "Creates utilities needed for the web application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import json\n",
        "\n",
        "# Save the necessary functions and components for the web app\n",
        "web_app_utils = {\n",
        "    'extract_faces_function': extract_faces_from_video,\n",
        "    'image_processor': image_processor,\n",
        "    'model_class': ConViTInspired,\n",
        "    'device': device\n",
        "}\n",
        "\n",
        "# Create a simplified prediction function for web app\n",
        "def create_web_prediction_function():\n",
        "\n",
        "    def predict_video_for_web(video_path, model, image_processor, device, num_frames=10):\n",
        "        frames = extract_faces_from_video(video_path, num_frames)\n",
        "        if not frames:\n",
        "            return {\"error\": \"No faces detected\"}\n",
        "        \n",
        "        predictions = []\n",
        "        confidences = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for frame in frames:\n",
        "                inputs = image_processor(frame, return_tensors='pt').pixel_values.to(device)\n",
        "                outputs = model(inputs)\n",
        "                logits = outputs.logits\n",
        "                probabilities = torch.softmax(logits, dim=1)\n",
        "                pred = torch.argmax(logits, dim=1).item()\n",
        "                confidence = torch.max(probabilities).item()\n",
        "                \n",
        "                predictions.append(pred)\n",
        "                confidences.append(confidence)\n",
        "        \n",
        "        # Calculate overall prediction\n",
        "        avg_pred = np.mean(predictions) > 0.5\n",
        "        avg_confidence = np.mean(confidences)\n",
        "        \n",
        "        return {\n",
        "            'overall_prediction': 'Fake' if avg_pred else 'Real',\n",
        "            'confidence': avg_confidence,\n",
        "            'frame_predictions': predictions,\n",
        "            'confidences': confidences,\n",
        "            'num_frames': len(predictions)\n",
        "        }\n",
        "    \n",
        "    return predict_video_for_web\n",
        "\n",
        "# Test the web prediction function\n",
        "web_predict = create_web_prediction_function()\n",
        "print(\"Web app utilities created successfully!\")\n",
        "print(\" Model saved: my_deepfake_model.pth\")\n",
        "print(\" Web prediction function created\")\n",
        "print(\" Ready for web application!\")\n",
        "\n",
        "# Save some metadata about the model\n",
        "model_info = {\n",
        "    'model_type': 'ConViTInspired',\n",
        "    'num_classes': 2,\n",
        "    'input_size': (224, 224),\n",
        "    'device': str(device),\n",
        "    'training_complete': True\n",
        "}\n",
        "\n",
        "with open('model_info.json', 'w') as f:\n",
        "    json.dump(model_info, f, indent=2)\n",
        "\n",
        "print(\"📄 Model info saved to model_info.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Thank You for Going Through\n",
        "Refer to ReadMe.md for any clarification on steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
